{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject-Group2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGxmGfMjXmcp"
      },
      "source": [
        "# **Deep Learning Project** - Aerial Perspective Object Detection\n",
        "SEP 769 - Group 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQmKNgA_GVld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32af1d7-87a1-4734-b273-d5f5490a58e8"
      },
      "source": [
        "#About semantic segmentation: https://www.jeremyjordan.me/semantic-segmentation/\n",
        "#TF Tutorial: https://www.tensorflow.org/tutorials/images/segmentation\n",
        "#TF load and process images: https://www.tensorflow.org/tutorials/load_data/images\n",
        "\n",
        "#Different image nets: https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
        "#TF Resnet: \n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "drive.mount('/content/gdrive')\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import fnmatch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/Shareddrives/SEP_769/data/drone_data/\"):\n",
        "  !unzip \"/content/gdrive/Shareddrives/SEP_769/drone_data.zip\" -d \"/content/gdrive/Shareddrives/SEP_769/data\"\n",
        "\n",
        "#image1 = cv2.imread('{}000.jpg'.format(original_images_path)) #original size image\n",
        "#image2 = cv2.resize(cv2.imread('{}000.jpg'.format(original_images_path), flags=1), (0,0), fx=0.05, fy=0.05)  #smaller image\n",
        "#print(image1.shape)\n",
        "#print(image2.shape)\n",
        "\n",
        "#print('\\nTotal of {} images at path: \\n{}\\n\\n'.format(image_count, original_images_path))\n",
        "\n",
        "#40 for test\n",
        "#360 for train\n",
        "\n",
        "#pixel accuracy, Intersection over Union (IOU)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBDOq0y71fY-"
      },
      "source": [
        "#*Constants and Functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnNjZk9Y1euR"
      },
      "source": [
        "original_images_path = '/content/gdrive/Shareddrives/SEP_769/data/drone_data/original_images/'\n",
        "original_semantic_path = '/content/gdrive/Shareddrives/SEP_769/data/drone_data/label_images_semantic/'\n",
        "HEIGHT = 200\n",
        "WIDTH = 300\n",
        "EPOCHS = 50\n",
        "BATCHSIZE = 3\n",
        "VAL_SPLIT = 0.9\n",
        "\n",
        "# reads images from dir, resizes and returns np array\n",
        "# optional file type argument ext\n",
        "def images_to_array(dir, width, height, ext='.jpg'):\n",
        "  data = []\n",
        "  files = os.listdir(dir)\n",
        "  files.sort()\n",
        "  for filename in files:\n",
        "    if filename.endswith(ext):\n",
        "      img = cv2.imread(dir+filename,flags=1)\n",
        "      img = cv2.resize(img, (width, height))\n",
        "      data.append(img)\n",
        "\n",
        "  return np.array(data)\n",
        "\n",
        "def labels_to_array(dir, width, height, ext='.jpg'):\n",
        "  data = []\n",
        "  files = os.listdir(dir)\n",
        "  files.sort()\n",
        "  for filename in files:\n",
        "    if filename.endswith(ext):\n",
        "      img = cv2.imread(dir+filename,flags=0)\n",
        "      img = cv2.resize(img, (width, height))\n",
        "      data.append(img)\n",
        "\n",
        "  return np.array(data)\n",
        "\n",
        "# displays a number of originals with their masks\n",
        "def display_images(images):\n",
        "  plt.figure(figsize=(15,10))\n",
        "  for i in range(len(images)):\n",
        "    plt.subplot(1, len(images), i + 1)\n",
        "    if len(images[i].shape) == 3:\n",
        "      plt.imshow(images[i])\n",
        "    else:\n",
        "      plt.imshow(images[i], cmap=\"gray\", vmin=0, vmax=255)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#plt.imshow(original_images[0])\n",
        "#plt.show()\n",
        "#print(original_images[0].shape)\n",
        "#plt.imshow(semantic_images[0])\n",
        "#plt.show()\n",
        "#print(semantic_images[0].shape) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpEq2q4K6g35"
      },
      "source": [
        "#Prepping Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxTrtxZu6fY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755d49f6-50e0-4ed2-b020-29a992261a79"
      },
      "source": [
        "images = images_to_array(original_images_path, width=WIDTH, height=HEIGHT)\n",
        "print(images.shape)\n",
        "\n",
        "labels = labels_to_array(original_semantic_path, width=WIDTH, height=HEIGHT, ext='png')\n",
        "print(labels.shape)\n",
        "\n",
        "\n",
        "\n",
        "#Old Image Import\n",
        "#original_images = [cv2.resize(cv2.imread(file, flags=1), (0,0), fx=0.05, fy=0.05) for file in glob.glob('{}*.jpg'.format(original_images_path))]  #importing images in color at 5% scale\n",
        "#semantic_images = [cv2.resize(cv2.imread(file, flags=0), (0,0), fx=0.05, fy=0.05) for file in glob.glob('{}*.png'.format(original_semantic_path))]  #importing images segmented\n",
        "#print(original_images.shape)\n",
        "#print(semantic_images.shape)\n",
        "\n",
        "#image_count = len(fnmatch.filter(os.listdir(original_images_path), '*.jpg'))\n",
        "#print('\\nTotal of {} images at path: \\n{}\\n\\n'.format(image_count, original_images_path))\n",
        "#image_count = len(fnmatch.filter(os.listdir(original_semantic_path), '*.png'))\n",
        "#print('\\nTotal of {} images at path: \\n{}\\n\\n'.format(image_count, original_semantic_path))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400, 200, 300, 3)\n",
            "(400, 200, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMLryDedHlOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e195165-46bc-45c5-d823-f6abccc48f41"
      },
      "source": [
        "train_images, test_images = tf.split(images, [360,40])\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n",
        "train_labels, test_labels = tf.split(labels, [360,40])\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "#try randomizing order of tran/test sets before split"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(360, 200, 300, 3)\n",
            "(40, 200, 300, 3)\n",
            "(360, 200, 300)\n",
            "(40, 200, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH6WuC_v-Xkb"
      },
      "source": [
        "#Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WXftyA9-cGS"
      },
      "source": [
        "for i in range(10):\n",
        "  display_images([images[i], labels[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiRZaYcwj6ws"
      },
      "source": [
        "#Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjq9tXS3j59z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "b7cf5fc1-6432-4fe4-9460-708ed0298558"
      },
      "source": [
        "input_Layer = tf.keras.Input(shape=(train_images[0].shape))\n",
        "\n",
        "resnet_layers = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=input_Layer)\n",
        "resnet_layers.trainable = False\n",
        "#resnet_layers.summary()\n",
        "\n",
        "new_layers = [\n",
        "  tf.keras.layers.UpSampling2D(),\n",
        "  tf.keras.layers.BatchNormalization,\n",
        "  tf.keras.layers.UpSampling2D(),\n",
        "  tf.keras.layers.BatchNormalization,\n",
        "  tf.keras.layers.UpSampling2D(),\n",
        "  tf.keras.layers.BatchNormalization,\n",
        "  tf.keras.layers.UpSampling2D(),\n",
        "  tf.keras.layers.BatchNormalization,\n",
        "  tf.keras.layers.UpSampling2D()\n",
        "]\n",
        "\n",
        "#def final_model():\n",
        "\n",
        "inputs = tf.keras.Input(shape=train_images[0].shape)\n",
        "x = inputs\n",
        "layer_skips = resnet_layers(x)\n",
        "x = layer_skips[-1]\n",
        "\n",
        "print(layer_skips)\n",
        "\n",
        "\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "'''\n",
        "layer_skips = reversed(layer_skips[:-1])   #why doesnt this work\n",
        "\n",
        "for up, skip in zip(resnet_layers, layer_skips):\n",
        "  x = up(x)\n",
        "  print(x)\n",
        "  concat = tf.keras.layers.Concatenate()\n",
        "  x = concat([x, skip])\n",
        "\n",
        "last_layer = tf.keras.layers.Conv2DTranspose(3, 3, strides=2, padding='same')\n",
        "\n",
        "x = last_layer(x)\n",
        "\n",
        "#return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "final_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "final_model.summary()\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-bf45d2e268e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_skips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_skips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute 'summary'"
          ]
        }
      ]
    }
  ]
}